{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ES 157` Section Y: Maximum Likelihood, MAP, and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We spent the last few weeks talking a lot about _maximum likelihood_ and the _maximum a posteriori_ estimators, as well as _principal component analysis_. We went through a lot of math during class and sections, so in this notebook we will spend some time visualizing concepts that we saw in class.\n",
    "\n",
    "At the end of this notebook you will\n",
    "1. have a better understanding of the maximum likelihood and MAP estimators,\n",
    "2. have seen how the MAP and maximum likelihood estimators are related, and\n",
    "3. have a better understanding of how PCA works and it's properties.\n",
    "\n",
    "As we always, let us import some needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Maximum Likelihood vs MAP ðŸ“™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During class and previous sections, we derived analytically both the maximum likelihood and the MAP estimators, for various settings, and we saw how they are related. However, here we would like to emphasize the _likelihood function_ and the _aposteriori function_ are, well, _functions_ of $\\theta$.\n",
    "\n",
    "For our setting, we will consider again the case of *Gaussian* i.i.d. random variables $X_1, \\ldots, X_n \\sim \\mathcal{N}(\\theta^{\\ast}, \\sigma^2)$, each generating a _single_ sample $x_1, \\ldots, x_n$. In what follows, we will try to estimate the _unknown_ mean of the random variables $\\theta^{\\ast}$.\n",
    "\n",
    "### Maximum Likelihood estimation\n",
    "When computing the maximum likelihood estimate, we make no assumption about the distribution of $\\theta^{\\ast}$. This basically means we have _no information_ about what $\\theta^{\\ast}$ \"looks like\", so we're solely using the data to find the best estimate. The likelihood function in this case, as we saw in class, is given by\n",
    "\n",
    "<center>$L(\\theta \\mid \\mathbf{x}) = \\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}} e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\theta)^2}$.</center>\n",
    "\n",
    "Then, the maximum likelihood estimator is given by\n",
    "\n",
    "<center>$\\hat{\\theta}_{\\textrm{ML}} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$.</center>\n",
    "\n",
    "Below, choose specific options for $\\theta^{\\ast}$ and $\\sigma^2$ and generate `n = 10` samples from that distribution. Plot the likelihood and log-likelihood functions over a range of values for $\\theta$ near the value that you chose. Overlay on your plots the true value $\\theta^{\\ast}$, along with the maximum likelihood estimate $\\hat{\\theta}_{\\textrm{ML}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your parameters\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the maximum likelihood ends up being, as expected, the maximum of the likelihood and/or the log-likelihood functions. Note that, as we stressed in class, the _point_ where the maximum is attained is the same for both the likelihood and the log-likelihood; as we discussed, _increasing_ functions might change the _value_ of the maximum, but not the point that attains it!\n",
    "\n",
    "### MAP estimation\n",
    "In MAP estimation, conversely, we make explicit assumptions about the distribution of $\\theta^{\\ast}$. Let us specifically assume that $\\theta^{\\ast} \\sim \\mathcal{N}(\\bar{\\theta}, \\tau^2)$. In layterms, this basically means that we know the mean is close to a number, say $5$, but we don't know it's exact value; it could be $5.12$ or $4.86$. Then, MAP estimation tries to strike a balance between \"trusting\" the data and using the prior information that we have. The posterior function in this case, as we saw in class, is given by\n",
    "\n",
    "<center>$p_{\\mathbf{X}}(\\theta \\mid \\mathbf{x}) = \\frac{p_{\\mathbf{X}}(\\mathbf{x} \\mid \\theta) \\cdot p_{\\theta}(\\theta)}{p_{\\mathbf{X}}(\\mathbf{x})}$,</center>\n",
    "\n",
    "where $p_{\\mathbf{X}}(\\mathbf{x} \\mid \\theta)$ is equal to the likelihood function $L(\\theta \\mid \\mathbf{x})$. In this case, the MAP estimator is given by\n",
    "\n",
    "<center>$\\hat{\\theta}_{\\textrm{MAP}} = \\frac{\\tau^2}{n \\tau^2 + \\sigma^2}\\sum_{i=1}^{n}x_i + \\frac{\\sigma^2}{n \\tau^2 + \\sigma^2}\\bar{\\theta}$.</center>\n",
    "\n",
    "Below, let $\\bar{\\theta}$ be equal to the value you chose for $\\theta^{\\ast}$ before, and choose a value for $\\tau^2$. Then, generate $\\theta^{\\ast}$ and sample `n = 10` samples from the data distribution. Plot the posterior and the log-posterior functions over a range of values for $\\theta$. Overlay on your plots the true value $\\theta^{\\ast}$, along with the MAP estimate $\\hat{\\theta}_{\\textrm{MAP}}$ and the maximum likelihood estimate $\\hat{\\theta}_{\\textrm{ML}}$. In your computations of the posterior, feel free to ignore the term $p_{\\mathbf{X}}(\\mathbf{x})$, i.e.\n",
    "<center>$p_{\\mathbf{X}}(\\theta \\mid \\mathbf{x}) = \\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}} e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\theta)^2} \\cdot \\frac{1}{\\sqrt{2 \\pi} \\tau} e^{-\\frac{1}{2\\tau^2}(\\theta-\\bar{\\theta})^2}$.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of $\\sigma^2$ and $\\tau^2$\n",
    "Having computed the MAP and ML estimates, let us examine now how they are affected by different choices of $\\sigma^2$ and $\\tau^2$. As a first exercise, plot the distribution of $\\theta^{\\ast} \\sim \\mathcal{N}(\\bar{\\theta}, \\tau^2)$ for different values of $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above describes the _prior distribution_ of $\\theta^{\\ast}$. In other words, it encodes the prior information we may have about $\\theta^{\\ast}$; when $\\tau$ is small, we are pretty confident that we have a good initial \"guess\" for $\\theta^{\\ast}$. Conversely, when $\\tau$ is large, virtually any $\\theta$ is equally likely to be the true value of $\\theta^{\\ast}$.\n",
    "\n",
    "Next, repeat the MAP and ML estimations for the two different values of $\\tau$ that are indicated. Also, plot the estimators again for a much higher value of $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vary tau\n",
    "theta_bar = 5\n",
    "sigma = 2\n",
    "taus = [0.01, 10]\n",
    "\n",
    "# high sigma\n",
    "sigma = 100\n",
    "tau = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We spent quite a few lectures and sections talking about PCA, and you implemented versions of it for `PSet 3` and `Lab 3`. In this notebook, we want to emphasize the steps of PCA, in a visual manner. Moreover, we will point out some of the common _pitfalls_ of PCA; namely, where and when it doesn't work as well as we'd hope.\n",
    "\n",
    "We will spare the excruciating details that we went over during class, as well as the exposition of the properties of the covariance matrix. We will only, for completeness, formally state the setting. Consider some data unlabelled $\\mathbf{X} \\in \\mathbb{R}^{n \\times m}$. Center the data and let $\\mathbf{X}_c = \\mathbf{X} - \\mathbb{E}(\\mathbf{X})$. Then, $\\boldsymbol{\\Sigma} = \\operatorname{Cov}(\\mathbf{X}_c)$ is a symmetric matrix, and has an eigenvalue decomposition, let $\\boldsymbol{\\Sigma} = \\mathbf{W} \\boldsymbol{\\Lambda} \\mathbf{W}^{-1}$, with $\\mathbf{W}\\mathbf{W}^T = \\mathbf{I}$. Then the columns of $\\mathbf{W}$ are called _principal directions_, and PCA is defined as the projection of the data on the principal components\n",
    "\n",
    "<center>$\\mathbf{Y} = \\mathbf{W}^T\\mathbf{X}_c.$</center>\n",
    "    \n",
    "PCA is an extremely powerful tool, most frequently used for _dimentionality reduction_. A few things to keep in mind about PCA:\n",
    "- PCA simply finds another basis to represent the data; namely, it finds the vectors along the directions with _maximal variance_. However, this is done in a _greedy_ manner, and not in a _joint_ maximization of the variance.\n",
    "- PCA creates an _orthonormal basis_ (which, in many cases, is a _pitfall_).\n",
    "As a final comment before we begin, we can think of PCA as changing the point from which we're looking at an object (we will come back to that perspective later on during the notebook).\n",
    "\n",
    "### Elementary PCA\n",
    "To set the setting, let's generate some data from an ellipse. In the following `code` cell, generate data that abides by the equation of an ellipse of your choice. To make things more interesting, rotate your data, and make sure that they are centered somewhere away from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of data points and parameters\n",
    "n = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now perform PCA. Our goal here is to plot the data and the principal directions after every step, to try and get a better understanding of exactly what PCA does. Let us again restate the steps of PCA, so we are all on the same page regarding what we need to do in what follows\n",
    "\n",
    "1. The first step towards PCA is centering our data around zero.\n",
    "2. Then, we compute the covariance matrix of the data.\n",
    "3. The _principal directions_ are defined as the eigenvectors of the covariance matrix.\n",
    "4. (Optional) We only keep a few coefficients.\n",
    "5. We project the data on the principal dimensions.\n",
    "6. To reconstruct, we \"invert\" the transformation to go back to the original domain.\n",
    "7. We re-add the mean to get the same representation.\n",
    "\n",
    "So, let's begin. ðŸ¤“ As the first step dictates, recenter your data and plot the zero-mean'ed and the original data on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to compute the _principal directions_. Again, these are simply the eigenvectors of the covariance matrix of the centered data. Compoute the principal directions, and plot them overlayed on both the original and the zero-meaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first principal direction is chosen along the axis with the _greatest_ variance. Then, the next direction of maximum variance is chosen, **but** under the constraint that it is _orthogonal_ to the first one. In the following `code` block, project both the original and the centered data on the principal components and plot everything on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the data on the principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that PCA generated axes along the directions that are minimizing the variance. Remember what we said; we can think of PCA as simply moving around the space and changing our point of view. Our original data was like a _frisbee_; all we did was change the viewpoint of the frisbee to see it in a clearer light.\n",
    "\n",
    "**Note**: with the risk of going on a tangent, this interpretation of PCA that we're introducing is not entirely arbitrary. On the contrary, by construction $\\mathbf{W}$ is an _orthonormal_ matrix. These matrices have a special place in algebra; the comprise the _special orthogonal group_ $SO(n)$. This group is also, aptly, called the _rotation group_; these matrices are transformations that generalize the notion of rotation to any dimension.\n",
    "\n",
    "Next, we will apply the \"inverse\" transformation to go back to the original domain. Note that in our simple example, we didn't really prune any of the dimensions, so the plot we expect to see is _identical_ to the one where we simply centered the data. In an actual application with _high-dimensional data_, we would only keep a few of the principal components before projecting back, resulting in a _low-dimensional_ approximation of the original data. Apply the \"inverse\" transformation and plot again the centered and the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"invert\" the projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, adding back the means of each dimension will get us, faithfully, back to our original data magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back the means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA pitfalls ðŸ‘Ž\n",
    "So far PCA seems pretty awesome! It is an extremely powerful tool, that is based on a very simple and intuitive idea, and is very easy to implement. There has to be a catch, _right_?\n",
    "\n",
    "As we mentioned before, the principal directions _have_ to be orthogonal to each other. Therefore, PCA will have rather poor performance when the data dimensions aren't orthogonal to each other.\n",
    "\n",
    "Another pitfall, in conjuction with the orthogonality constraint, is that PCA maximizes variance in a _greedy_ manner. What that means is that it chooses the direction of maximum variance, and _then_ chooses another direction orthogonal to that. However, if the directions (again, baring the constraint of orthogonality) were chosen _jointly_, rather than _sequentially_, we could minimize the overall variance of the data.\n",
    "\n",
    "Let us try to illustrate these two pitfalls in conjuction, by showing an example where the principal directions chosen by PCA seem like a rather poor choice. To that end, generate again data from _two_, this time, ellipses so that they create an overall \"X\" shaped pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of data points and parameters\n",
    "n = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why is this particular dataset interesting? Well, we see that the data lie along directions that are not orthogonal, and still have a decent amount of variance along those directions. What do you expect the principal directions will look like for the above dataset? Compute the principal directions below, and overlay them in both the centered and the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero mean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, were you expecting these principal directions? ðŸ¤” We see that, even though these directions are orthogonal to each other and are along the directions of maximal variance, they are not very good for modeling the data. In the next `code` block, project the data onto the principal dimensions so we can have a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the data on the principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the axes don't really align with the parts of the data that we would expect; this is an example of a dataset where PCA performs poorly. Some of the reasons for this are, as we mentioned, that PCA enforces orthogonality on the representation, and also the principal directions are chosen in a greedy manner.\n",
    "\n",
    "However, another reason why PCA underperforms is that it doesn't, in any way, try to optimize the _representation_ of the data; instead, it simply tries to optimize the variance. There are other approahces, such as _dictionary learning_ that explicitly try to optimize the data representation.\n",
    "\n",
    "I hope you enjoyed this week's notebook. Please take a minute to fill out the feedback [form](https://docs.google.com/forms/d/e/1FAIpQLSdL0OU3As1n81EmXayPrCiiQOTh8RTgLGVvIoVwUPh94uRK4Q/viewform?usp=sf_link)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
